{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import sklearn.datasets\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from pandas.api.types import CategoricalDtype\r\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, StratifiedKFold, cross_val_score\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.ensemble import BaggingClassifier\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.ensemble import AdaBoostClassifier\r\n",
    "from sklearn.ensemble import GradientBoostingClassifier\r\n",
    "from autorank import autorank, create_report\r\n",
    "\r\n",
    "# Render matplotlib plots in the notebook\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "PATH_CAE = Path('caesarian.csv')\r\n",
    "CAE = pd.read_csv(PATH_CAE)\r\n",
    "#CAE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "PATH_ARR = Path('arrhythmia.csv')\r\n",
    "ARR = pd.read_csv(PATH_ARR)\r\n",
    "for col in ARR.columns:\r\n",
    "    ARR[col] = pd.to_numeric(ARR[col],errors='coerce')\r\n",
    "    ARR = ARR.replace(np.nan, ARR[col].mean(), regex=True)\r\n",
    "    ARR[col] = ARR[col].astype(int)\r\n",
    "#ARR.iloc[:, 14]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "PATH_WEB = Path('website-phishing.csv')\r\n",
    "WEB = pd.read_csv(PATH_WEB)\r\n",
    "WEB.columns = WEB.columns.map(str.strip)\r\n",
    "#WEB"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validation on Caesarian data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "y = CAE['class'].to_numpy()\r\n",
    "X = CAE.drop(['class'], axis=1).to_numpy()\r\n",
    "\r\n",
    "print(X.shape, y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(80, 5) (80,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "models = []\r\n",
    "models.append(('Decision Tree', DecisionTreeClassifier())) \r\n",
    "models.append(('Random Forest', RandomForestClassifier(n_estimators=10))) \r\n",
    "models.append(('Bagging', BaggingClassifier(n_estimators=10)))\r\n",
    "models.append(('Ada Boost', AdaBoostClassifier(n_estimators=10)))\r\n",
    "models.append(('Gradient Boost', GradientBoostingClassifier(n_estimators=10)))\r\n",
    "\r\n",
    "lst = []\r\n",
    "algo = list(list(zip(*models))[0])\r\n",
    "results = pd.DataFrame(columns=algo)\r\n",
    "for name, model in models:\r\n",
    "    kfold = RepeatedKFold(n_splits=10, n_repeats = 100 , random_state = 42)\r\n",
    "    scores = cross_val_score(model, X, y, cv=kfold)\r\n",
    "    spl = np.split(scores,100)\r\n",
    "    for i, run in enumerate(spl):\r\n",
    "        run_mean = np.mean(run)\r\n",
    "        run_std = np.std(run)\r\n",
    "        #print(f\"Test run accuracy for {name} is: {run_mean:.3f}  {run_std:.3f}\")\r\n",
    "        results.at[i, name] = run_mean\r\n",
    "    mean = np.mean(scores)\r\n",
    "    std = np.std(scores)\r\n",
    "    #print(scores)\r\n",
    "    #print(len(scores))\r\n",
    "    #print(spl)\r\n",
    "    print(f\"Mean test accuracy for {name} after 100 runs on caesarian dataset: {mean:.3f} with std of {std:.3f}\")\r\n",
    "    lst.append(mean)\r\n",
    "#print(lst)\r\n",
    "print(results)\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean test accuracy for Decision Tree after 100 runs on caesarian dataset: 0.523 with std of 0.172\n",
      "Mean test accuracy for Random Forest after 100 runs on caesarian dataset: 0.544 with std of 0.170\n",
      "Mean test accuracy for Bagging after 100 runs on caesarian dataset: 0.528 with std of 0.171\n",
      "Mean test accuracy for Ada Boost after 100 runs on caesarian dataset: 0.612 with std of 0.162\n",
      "Mean test accuracy for Gradient Boost after 100 runs on caesarian dataset: 0.606 with std of 0.168\n",
      "   Decision Tree Random Forest Bagging Ada Boost Gradient Boost\n",
      "0         0.5125          0.55   0.525    0.6375            0.6\n",
      "1         0.4875        0.5625  0.4875       0.6         0.5875\n",
      "2         0.5125        0.5375   0.525     0.575            0.6\n",
      "3         0.5125         0.575   0.525    0.6125          0.625\n",
      "4            0.6           0.6  0.4875      0.65         0.6625\n",
      "..           ...           ...     ...       ...            ...\n",
      "95        0.5125          0.45  0.4875    0.5875         0.5875\n",
      "96         0.525          0.55  0.5125     0.575          0.625\n",
      "97        0.5375         0.575     0.5     0.625         0.5625\n",
      "98         0.475          0.45  0.4875    0.6125            0.6\n",
      "99        0.5375        0.6125  0.5875     0.625          0.625\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Auto Rank data from running algorithm on Caesarian "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "results= results.astype(float, errors = 'raise')\r\n",
    "result = autorank(results)\r\n",
    "print(result)\r\n",
    "create_report(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RankResult(rankdf=\n",
      "                meanrank      mean       std  ci_lower  ci_upper effect_size  \\\n",
      "Ada Boost          1.555  0.611875  0.024770  0.605369  0.618381         0.0   \n",
      "Gradient Boost     1.735  0.605500  0.036641  0.595877  0.615123    0.203844   \n",
      "Random Forest      3.470  0.544500  0.042615  0.533308  0.555692    1.933068   \n",
      "Bagging            4.010  0.528000  0.038522  0.517882  0.538118    2.589967   \n",
      "Decision Tree      4.230  0.523000  0.036050  0.513532  0.532468    2.873537   \n",
      "\n",
      "                 magnitude  \n",
      "Ada Boost       negligible  \n",
      "Gradient Boost       small  \n",
      "Random Forest        large  \n",
      "Bagging              large  \n",
      "Decision Tree        large  \n",
      "pvalue=3.6137582352592976e-56\n",
      "cd=0.6099436402016305\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.16704486310482025, 0.012622312642633915, 0.35619452595710754, 0.021136658266186714, 0.07734933495521545]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=7.51383973885904e-06\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=100\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n",
      "The statistical analysis was conducted for 5 populations with 100 paired samples.\n",
      "The family-wise significance level of the tests is alpha=0.050.\n",
      "We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.013). Therefore, we assume that all populations are normal.\n",
      "We applied Bartlett's test for homogeneity and reject the null hypothesis (p=0.000) that thedata is homoscedastic. Thus, we assume that our data is heteroscedastic.\n",
      "Because we have more than two populations and the populations are normal but heteroscedastic, we use the non-parametric Friedman test as omnibus test to determine if there are any significant differences between the mean values of the populations. We use the post-hoc Nemenyi test to infer which differences are significant. We report the mean value (M), the standard deviation (SD) and the mean rank (MR) among all populations over the samples. Differences between populations are significant, if the difference of the mean rank is greater than the critical distance CD=0.610 of the Nemenyi test.\n",
      "We reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations Ada Boost (M=0.612+-0.007, SD=0.025, MR=1.555), Gradient Boost (M=0.605+-0.010, SD=0.037, MR=1.735), Random Forest (M=0.544+-0.011, SD=0.043, MR=3.470), Bagging (M=0.528+-0.010, SD=0.039, MR=4.010), and Decision Tree (M=0.523+-0.009, SD=0.036, MR=4.230). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\n",
      "Based on the post-hoc Nemenyi test, we assume that there are no significant differences within the following groups: Ada Boost and Gradient Boost; Random Forest and Bagging; Bagging and Decision Tree. All other differences are significant.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Validation on Phishing data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "y = WEB['Class'].to_numpy()\r\n",
    "X = WEB.drop(['Class'], axis=1).to_numpy()\r\n",
    "\r\n",
    "print(X.shape, y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(11055, 30) (11055,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "models = []\r\n",
    "models.append(('Decision Tree', DecisionTreeClassifier())) \r\n",
    "models.append(('Random Forest', RandomForestClassifier(n_estimators=10))) \r\n",
    "models.append(('Bagging', BaggingClassifier(n_estimators=10)))\r\n",
    "models.append(('Ada Boost', AdaBoostClassifier(n_estimators=10)))\r\n",
    "models.append(('Gradient Boost', GradientBoostingClassifier(n_estimators=10)))\r\n",
    "\r\n",
    "lst = []\r\n",
    "algo = list(list(zip(*models))[0])\r\n",
    "results = pd.DataFrame(columns=algo)\r\n",
    "for name, model in models:\r\n",
    "    kfold = RepeatedKFold(n_splits=10, n_repeats = 100 , random_state = 42)\r\n",
    "    scores = cross_val_score(model, X, y, cv=kfold)\r\n",
    "    spl = np.split(scores,100)\r\n",
    "    for i, run in enumerate(spl):\r\n",
    "        run_mean = np.mean(run)\r\n",
    "        run_std = np.std(run)\r\n",
    "        #print(f\"Test run accuracy for {name} is: {run_mean:.3f}  {run_std:.3f}\")\r\n",
    "        results.at[i, name] = run_mean\r\n",
    "    mean = np.mean(scores)\r\n",
    "    std = np.std(scores)\r\n",
    "    #print(len(scores))\r\n",
    "    print(f\"Mean test accuracy for {name} after 100 runs on phishing dataset: {mean:.3f} with std of {std:.3f}\")\r\n",
    "    lst.append(mean)\r\n",
    "#print(lst)\r\n",
    "print(results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean test accuracy for Decision Tree after 100 runs on phishing dataset: 0.965 with std of 0.005\n",
      "Mean test accuracy for Random Forest after 100 runs on phishing dataset: 0.971 with std of 0.005\n",
      "Mean test accuracy for Bagging after 100 runs on phishing dataset: 0.970 with std of 0.005\n",
      "Mean test accuracy for Ada Boost after 100 runs on phishing dataset: 0.929 with std of 0.007\n",
      "Mean test accuracy for Gradient Boost after 100 runs on phishing dataset: 0.917 with std of 0.008\n",
      "   Decision Tree Random Forest   Bagging Ada Boost Gradient Boost\n",
      "0       0.963184      0.970511   0.97042  0.928811       0.917232\n",
      "1        0.96644      0.972229   0.97006  0.929173       0.917231\n",
      "2       0.964722      0.971416  0.968431  0.929173       0.917049\n",
      "3       0.964722      0.970149   0.97033  0.929534       0.916778\n",
      "4       0.963546      0.972049  0.969154  0.929443       0.917051\n",
      "..           ...           ...       ...       ...            ...\n",
      "95       0.96436      0.970782  0.968521   0.92872       0.917144\n",
      "96      0.963817      0.970149  0.967616  0.929082       0.917141\n",
      "97      0.966351       0.97232  0.971054  0.929807       0.917052\n",
      "98       0.96635      0.970873  0.969788  0.929444       0.917141\n",
      "99       0.96653      0.972591  0.970059  0.928808       0.917231\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Auto Rank data from running algorithm on Phishing "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "results= results.astype(float, errors = 'raise')\r\n",
    "result = autorank(results)\r\n",
    "print(result)\r\n",
    "create_report(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RankResult(rankdf=\n",
      "                meanrank    median       mad  ci_lower  ci_upper effect_size  \\\n",
      "Random Forest       1.05  0.971234  0.001073  0.970873  0.972047         0.0   \n",
      "Bagging             1.95  0.969653  0.001005  0.969245   0.97006    1.521865   \n",
      "Decision Tree       3.00  0.965264   0.00134  0.964722  0.965989    4.919494   \n",
      "Ada Boost           4.00  0.929218  0.000338   0.92908  0.929441   52.825585   \n",
      "Gradient Boost      5.00  0.917140   0.00013  0.917052  0.917142   70.794976   \n",
      "\n",
      "                 magnitude  \n",
      "Random Forest   negligible  \n",
      "Bagging              large  \n",
      "Decision Tree        large  \n",
      "Ada Boost            large  \n",
      "Gradient Boost       large  \n",
      "pvalue=1.8421898918723992e-84\n",
      "cd=0.6099436402016305\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[0.4343690574169159, 0.3878830671310425, 0.6997711062431335, 0.13431629538536072, 9.768938724619147e-08]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=3.6978656331873236e-39\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=100\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n",
      "The statistical analysis was conducted for 5 populations with 100 paired samples.\n",
      "The family-wise significance level of the tests is alpha=0.050.\n",
      "We rejected the null hypothesis that the population is normal for the population Gradient Boost (p=0.000). Therefore, we assume that not all populations are normal.\n",
      "Because we have more than two populations and the populations and one of them is not normal, we use the non-parametric Friedman test as omnibus test to determine if there are any significant differences between the median values of the populations. We use the post-hoc Nemenyi test to infer which differences are significant. We report the median (MD), the median absolute deviation (MAD) and the mean rank (MR) among all populations over the samples. Differences between populations are significant, if the difference of the mean rank is greater than the critical distance CD=0.610 of the Nemenyi test.\n",
      "We reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations Random Forest (MD=0.971+-0.001, MAD=0.001, MR=1.050), Bagging (MD=0.970+-0.000, MAD=0.001, MR=1.950), Decision Tree (MD=0.965+-0.001, MAD=0.001, MR=3.000), Ada Boost (MD=0.929+-0.000, MAD=0.000, MR=4.000), and Gradient Boost (MD=0.917+-0.000, MAD=0.000, MR=5.000). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\n",
      "Based on the post-hoc Nemenyi test, we assume that all differences between the populations are significant.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross validation on Arrythmia data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "y = ARR['class'].to_numpy()\r\n",
    "X = ARR.drop(['class'], axis=1).to_numpy()\r\n",
    "\r\n",
    "print(X.shape, y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(452, 279) (452,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "models = []\r\n",
    "models.append(('Decision Tree', DecisionTreeClassifier())) \r\n",
    "models.append(('Random Forest', RandomForestClassifier(n_estimators=10))) \r\n",
    "models.append(('Bagging', BaggingClassifier(n_estimators=10)))\r\n",
    "models.append(('Ada Boost', AdaBoostClassifier(n_estimators=10)))\r\n",
    "models.append(('Gradient Boost', GradientBoostingClassifier(n_estimators=10)))\r\n",
    "\r\n",
    "lst = []\r\n",
    "algo = list(list(zip(*models))[0])\r\n",
    "results = pd.DataFrame(columns=algo)\r\n",
    "for name, model in models:\r\n",
    "    kfold = RepeatedKFold(n_splits=10, n_repeats = 100 , random_state = 42)\r\n",
    "    scores = cross_val_score(model, X, y, cv=kfold)\r\n",
    "    spl = np.split(scores,100)\r\n",
    "    for i, run in enumerate(spl):\r\n",
    "        run_mean = np.mean(run)\r\n",
    "        run_std = np.std(run)\r\n",
    "        #print(f\"Test run accuracy for {name} is: {run_mean:.3f}  {run_std:.3f}\")\r\n",
    "        results.at[i, name] = run_mean\r\n",
    "    mean = np.mean(scores)\r\n",
    "    std = np.std(scores)\r\n",
    "    #print(len(scores))\r\n",
    "    print(f\"Mean test accuracy for {name} after 100 runs on arrhythmia dataset: {mean:.3f} with std of {std:.3f}\")\r\n",
    "    lst.append(mean)\r\n",
    "#print(lst)\r\n",
    "print(results)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean test accuracy for Decision Tree after 100 runs on arrhythmia dataset: 0.619 with std of 0.067\n",
      "Mean test accuracy for Random Forest after 100 runs on arrhythmia dataset: 0.688 with std of 0.064\n",
      "Mean test accuracy for Bagging after 100 runs on arrhythmia dataset: 0.726 with std of 0.065\n",
      "Mean test accuracy for Ada Boost after 100 runs on arrhythmia dataset: 0.610 with std of 0.069\n",
      "Mean test accuracy for Gradient Boost after 100 runs on arrhythmia dataset: 0.703 with std of 0.065\n",
      "   Decision Tree Random Forest   Bagging Ada Boost Gradient Boost\n",
      "0       0.654831      0.701401  0.719082  0.617391       0.736908\n",
      "1       0.608599      0.681159      0.73  0.621836       0.692464\n",
      "2       0.628213      0.698889  0.734396  0.617053       0.716667\n",
      "3       0.619614      0.672947  0.703768  0.613043       0.677005\n",
      "4        0.62599      0.687923   0.72314  0.601643        0.71657\n",
      "..           ...           ...       ...       ...            ...\n",
      "95      0.619614      0.668309  0.723285  0.619469       0.697005\n",
      "96      0.602174      0.674686  0.712415  0.605942       0.725797\n",
      "97      0.621691      0.683671  0.727971  0.610725       0.705942\n",
      "98      0.634879      0.714686  0.719179  0.615024        0.70599\n",
      "99      0.613092      0.697198  0.719372  0.606473       0.686039\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Auto Rank data from running algorithm on Arrythmia"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "results= results.astype(float, errors = 'raise')\r\n",
    "result = autorank(results)\r\n",
    "print(result)\r\n",
    "create_report(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RankResult(rankdf=\n",
      "                meanrank    median       mad  ci_lower  ci_upper effect_size  \\\n",
      "Bagging            1.130  0.725676   0.01035  0.721063  0.732222         0.0   \n",
      "Gradient Boost     2.045  0.702512  0.011388  0.697053  0.706135    2.128829   \n",
      "Random Forest      2.825  0.687850  0.012641  0.683671  0.692657    3.274277   \n",
      "Decision Tree      4.300  0.619589  0.016187  0.610966  0.626232    7.808886   \n",
      "Ada Boost          4.700  0.610725  0.003653  0.608406   0.61285   14.812082   \n",
      "\n",
      "                 magnitude  \n",
      "Bagging         negligible  \n",
      "Gradient Boost       large  \n",
      "Random Forest        large  \n",
      "Decision Tree        large  \n",
      "Ada Boost            large  \n",
      "pvalue=7.530518001386896e-77\n",
      "cd=0.6099436402016305\n",
      "omnibus=friedman\n",
      "posthoc=nemenyi\n",
      "all_normal=False\n",
      "pvals_shapiro=[0.5723743438720703, 0.011801826767623425, 0.7937547564506531, 0.0014996429672464728, 0.06660961359739304]\n",
      "homoscedastic=False\n",
      "pval_homogeneity=2.8441753164723372e-12\n",
      "homogeneity_test=levene\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=100\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=akinshin_gamma)\n",
      "The statistical analysis was conducted for 5 populations with 100 paired samples.\n",
      "The family-wise significance level of the tests is alpha=0.050.\n",
      "We rejected the null hypothesis that the population is normal for the population Decision Tree (p=0.001). Therefore, we assume that not all populations are normal.\n",
      "Because we have more than two populations and the populations and one of them is not normal, we use the non-parametric Friedman test as omnibus test to determine if there are any significant differences between the median values of the populations. We use the post-hoc Nemenyi test to infer which differences are significant. We report the median (MD), the median absolute deviation (MAD) and the mean rank (MR) among all populations over the samples. Differences between populations are significant, if the difference of the mean rank is greater than the critical distance CD=0.610 of the Nemenyi test.\n",
      "We reject the null hypothesis (p=0.000) of the Friedman test that there is no difference in the central tendency of the populations Bagging (MD=0.726+-0.006, MAD=0.010, MR=1.130), Gradient Boost (MD=0.703+-0.005, MAD=0.011, MR=2.045), Random Forest (MD=0.688+-0.004, MAD=0.013, MR=2.825), Decision Tree (MD=0.620+-0.008, MAD=0.016, MR=4.300), and Ada Boost (MD=0.611+-0.002, MAD=0.004, MR=4.700). Therefore, we assume that there is a statistically significant difference between the median values of the populations.\n",
      "Based on the post-hoc Nemenyi test, we assume that there are no significant differences within the following groups: Decision Tree and Ada Boost. All other differences are significant.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "deca9b341bfe840aacf908b00e24804a7f259194fc988101edf12a473132e09e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
